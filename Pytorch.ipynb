{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03581fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0579, 0.7794, 0.7660],\n",
      "        [0.4059, 0.5663, 0.2166],\n",
      "        [0.3554, 0.9607, 0.4781],\n",
      "        [0.9218, 0.9775, 0.0142],\n",
      "        [0.3850, 0.5911, 0.0251]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26cffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in e:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (20.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.0.19)\n",
      "Requirement already satisfied: requests in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.25.1)\n",
      "Requirement already satisfied: filelock in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.59.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.19.5)\n",
      "Requirement already satisfied: torch>=1.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in e:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers[torch]) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[torch]) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: click in e:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers[torch]) (7.1.2)\n",
      "Requirement already satisfied: six in e:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers[torch]) (1.15.0)\n",
      "Requirement already satisfied: joblib in e:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers[torch]) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\Users\\HP\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6910ed62",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-5cd0132a72a9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-5cd0132a72a9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97347d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in e:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: requests in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: filelock in e:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in e:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in e:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in e:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in e:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\Users\\HP\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a419c343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cad44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998743534088135}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('i am happy about this code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c34d40d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: POSITIVE, with score: 0.9534\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"its a pleasure to know data science.\",\n",
    "           \"hope i don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ea2a32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-27e7a0be5077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"distilbert-base-uncased-finetuned-sst-2-english\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         raise ValueError(\n\u001b[0;32m    421\u001b[0m             \u001b[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[0;32m   1393\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m                     \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1403\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1624\u001b[0m                     )\n\u001b[0;32m   1625\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1626\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m   1627\u001b[0m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07dd00f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.1-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six~=1.15.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "Collecting clang~=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accès refusé: 'E:\\\\Users\\\\HP\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\Users\\HP\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: wheel~=0.35 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.1-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.2-py2.py3-none-any.whl (155 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (58.2.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py): started\n",
      "  Building wheel for clang (setup.py): finished with status 'done'\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=ec44fd7cab30fb78d6939b604d6dba3ab59fbda05dd44271e90f7b4c05a0ea2b\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\f1\\60\\77\\22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=4a4b1173447d4dde12c7b7c8d08448e3d6d9d9d6a8014d0336c6d6bfd0bf207e\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849c5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# Initializing a model from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4761055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, is this working\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc4a34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_logits = outputs.prediction_logits\n",
    "seq_relationship_logits = outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe616720",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFAutoModelForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-27e7a0be5077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"distilbert-base-uncased-finetuned-sst-2-english\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\utils\\dummy_tf_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__name__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nTFAutoModelForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea6113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"is this working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3421bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2003, 2023, 2551, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d723b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bcf3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995556473731995}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('This code is not simple.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa97498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55dc3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e3035638174a8fa3fb13960fc855bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/638M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bac38a606144ba980bd941d209e542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a63c260a344a43be8875f6e5c5c83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc07e2e00ad451593f327117e1b135e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8acdbada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.35580480098724365}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"je suis une travailleuse acharnée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a85603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '3 stars', 'score': 0.5331571102142334}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Soy un poco trabajador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77a54ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my code is working\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db87cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
      "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
      "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
      "         ...,\n",
      "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
      "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
      "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1946e-01, -2.1445e-01, -2.9576e-01,  3.6603e-01,  2.7968e-01,\n",
      "          2.2183e-02,  5.7299e-01,  6.2331e-02,  5.9585e-02, -9.9965e-01,\n",
      "          5.0145e-02,  4.4756e-01,  9.7612e-01,  3.3989e-02,  8.4494e-01,\n",
      "         -3.6905e-01,  9.8648e-02, -3.7169e-01,  1.7371e-01,  1.1515e-01,\n",
      "          4.4133e-01,  9.9525e-01,  3.7221e-01,  8.2881e-02,  2.1402e-01,\n",
      "          6.8965e-01, -6.1042e-01,  8.7136e-01,  9.4158e-01,  5.7372e-01,\n",
      "         -3.2187e-01,  8.6671e-03, -9.8611e-01, -2.0542e-02, -4.3756e-01,\n",
      "         -9.8012e-01,  1.1142e-01, -6.7587e-01,  1.3499e-01,  3.1130e-01,\n",
      "         -8.2997e-01,  1.9006e-01,  9.9896e-01, -3.1798e-01,  2.1518e-02,\n",
      "         -1.6531e-01, -9.9943e-01,  1.0173e-01, -8.1811e-01,  3.3121e-02,\n",
      "          3.6740e-01, -7.3228e-02, -1.4261e-01,  1.8907e-01,  2.6119e-01,\n",
      "          4.1582e-01, -2.4427e-01, -5.9846e-02, -7.3492e-02, -3.4202e-01,\n",
      "         -5.8001e-01,  2.8331e-01, -5.0513e-01, -8.1967e-01,  1.9814e-01,\n",
      "          1.9108e-01,  3.7011e-02, -1.1327e-01,  1.3472e-01, -2.1614e-01,\n",
      "          6.3494e-01,  2.4869e-02,  3.8287e-01, -8.1779e-01, -2.4874e-01,\n",
      "          8.4982e-02, -5.2998e-01,  1.0000e+00, -5.2155e-02, -9.7052e-01,\n",
      "          3.9848e-01,  2.1361e-02,  3.9035e-01,  3.5588e-01, -1.7881e-01,\n",
      "         -9.9997e-01,  2.6939e-01, -3.8057e-02, -9.8657e-01,  6.9322e-02,\n",
      "          3.9138e-01, -2.1884e-02, -9.6330e-02,  3.8545e-01, -3.4136e-01,\n",
      "         -8.0363e-02, -3.2022e-02, -3.6328e-01, -7.8130e-02,  1.9191e-02,\n",
      "         -1.3429e-01, -1.6013e-02, -5.2640e-02, -2.8006e-01,  9.3612e-02,\n",
      "         -2.2885e-01, -1.2305e-01, -1.1002e-01, -3.2808e-01,  4.0356e-01,\n",
      "          2.8048e-01, -2.0102e-01,  2.7685e-01, -9.4023e-01,  4.1756e-01,\n",
      "         -1.5473e-01, -9.7553e-01, -4.3003e-01, -9.8546e-01,  5.9158e-01,\n",
      "          3.7343e-02, -1.9320e-01,  9.1691e-01,  3.6011e-01,  1.4505e-01,\n",
      "          1.5398e-01, -1.0659e-02, -1.0000e+00, -3.1573e-01, -3.1038e-01,\n",
      "          1.6523e-01, -8.0330e-02, -9.6650e-01, -9.4546e-01,  3.6145e-01,\n",
      "          9.0138e-01, -7.2696e-02,  9.9774e-01,  3.7289e-02,  9.3599e-01,\n",
      "          2.5317e-01, -2.0185e-01,  2.9534e-02, -2.3162e-01,  3.4632e-01,\n",
      "         -1.0763e-01, -2.6565e-01,  1.0874e-01,  1.2985e-01,  2.1135e-02,\n",
      "         -9.6284e-02, -7.6358e-02, -6.5151e-02, -8.9277e-01, -2.3465e-01,\n",
      "          9.1176e-01,  7.0428e-02, -2.1429e-01,  3.8197e-01,  3.5892e-02,\n",
      "         -1.6971e-01,  7.0654e-01,  2.4045e-01,  1.5014e-01, -1.9478e-02,\n",
      "          2.1369e-01, -1.7977e-01,  3.5112e-01, -6.0260e-01,  4.1683e-01,\n",
      "          1.8090e-01, -3.2497e-02, -3.0138e-01, -9.7103e-01, -1.3917e-01,\n",
      "          3.5130e-01,  9.8326e-01,  5.2702e-01,  4.8812e-02,  1.3992e-02,\n",
      "         -6.7964e-02,  2.9718e-01, -9.4136e-01,  9.7219e-01, -2.4774e-02,\n",
      "          1.5224e-01, -1.8241e-01,  5.5585e-02, -7.7306e-01, -9.8999e-02,\n",
      "          4.7058e-01, -1.7023e-01, -7.7803e-01,  5.2833e-02, -3.7679e-01,\n",
      "         -4.1296e-02, -4.9612e-01,  1.4171e-01, -1.1803e-01, -1.8995e-01,\n",
      "          5.0383e-02,  9.0623e-01,  7.8828e-01,  5.2288e-01, -3.5274e-01,\n",
      "          2.8563e-01, -8.1494e-01, -1.9622e-01, -9.2975e-02,  5.9311e-02,\n",
      "          3.1903e-02,  9.8860e-01, -3.9452e-01,  1.1867e-01, -8.6977e-01,\n",
      "         -9.7789e-01, -1.4859e-01, -7.7064e-01, -4.0620e-03, -4.1152e-01,\n",
      "          3.2578e-01,  1.8777e-01, -2.4501e-01,  2.6668e-01, -7.9329e-01,\n",
      "         -4.8133e-01,  9.3245e-02, -1.7010e-01,  2.7043e-01, -3.5880e-02,\n",
      "          7.7973e-01,  4.6697e-01, -3.4636e-01,  5.5237e-02,  9.0312e-01,\n",
      "         -2.4115e-01, -6.4200e-01,  4.1441e-01, -9.7797e-02,  6.2983e-01,\n",
      "         -4.1787e-01,  9.4069e-01,  4.9285e-01,  3.6058e-01, -8.7901e-01,\n",
      "         -2.6726e-01, -5.4679e-01,  9.3848e-04, -1.0502e-02, -4.6837e-01,\n",
      "          3.1116e-01,  3.6999e-01,  1.3306e-01,  6.4092e-01, -3.5630e-01,\n",
      "          8.8549e-01, -8.9036e-01, -9.3865e-01, -8.1215e-01,  2.7362e-01,\n",
      "         -9.8566e-01,  4.0363e-01,  2.1223e-01, -1.4316e-01, -2.4553e-01,\n",
      "         -2.1144e-01, -9.4728e-01,  5.0806e-01, -9.6621e-02,  8.5571e-01,\n",
      "         -1.0133e-01, -6.7768e-01, -2.8500e-01, -8.9905e-01, -3.3577e-01,\n",
      "          8.9155e-02,  3.2600e-01, -2.6467e-01, -9.2032e-01,  3.4629e-01,\n",
      "          3.3430e-01,  2.1397e-01,  3.0628e-02,  9.3878e-01,  9.9986e-01,\n",
      "          9.6385e-01,  8.3159e-01,  6.2250e-01, -9.8055e-01, -7.3623e-01,\n",
      "          9.9986e-01, -7.8395e-01, -9.9998e-01, -8.7800e-01, -5.0893e-01,\n",
      "          2.3399e-02, -1.0000e+00, -6.1938e-02,  1.9563e-01, -9.0552e-01,\n",
      "         -1.4008e-01,  9.5264e-01,  7.9837e-01, -1.0000e+00,  7.6343e-01,\n",
      "          8.3670e-01, -4.5859e-01,  5.4410e-01, -2.4073e-01,  9.6085e-01,\n",
      "          1.9164e-01,  3.2135e-01, -1.3064e-02,  2.4534e-01, -5.3001e-01,\n",
      "         -5.9538e-01,  3.7464e-01, -2.1190e-01,  8.8024e-01,  1.9648e-02,\n",
      "         -3.8349e-01, -8.4779e-01,  1.4677e-02, -2.8376e-02, -4.4313e-01,\n",
      "         -9.4966e-01, -6.5704e-02, -7.2325e-02,  6.5967e-01, -1.1504e-01,\n",
      "          2.1876e-01, -5.5254e-01,  9.2219e-02, -5.0583e-01, -5.2826e-02,\n",
      "          5.1425e-01, -8.9533e-01, -1.2744e-01,  9.7845e-02, -6.0145e-01,\n",
      "         -3.1653e-02, -9.5186e-01,  9.4685e-01, -2.2341e-01,  1.8390e-01,\n",
      "          1.0000e+00,  1.1756e-01, -7.0390e-01,  3.2502e-01, -1.0898e-02,\n",
      "         -1.8308e-01,  9.9999e-01,  5.8376e-01, -9.7387e-01, -3.3783e-01,\n",
      "          2.9640e-01, -2.7002e-01, -2.2243e-01,  9.9711e-01,  1.4422e-02,\n",
      "          7.8267e-02,  3.8660e-01,  9.7787e-01, -9.8501e-01,  8.7459e-01,\n",
      "         -7.2276e-01, -9.5249e-01,  9.4567e-01,  9.1005e-01, -5.0722e-01,\n",
      "         -4.9026e-01, -1.2517e-01, -3.9076e-02,  8.8128e-02, -8.2481e-01,\n",
      "          3.8301e-01,  1.8045e-01,  5.4796e-02,  8.0041e-01, -3.3501e-01,\n",
      "         -3.9115e-01,  1.4233e-01, -9.0142e-02,  3.4585e-01,  4.4044e-01,\n",
      "          3.1045e-01, -1.3280e-01, -1.3614e-01, -3.0303e-01, -4.8794e-01,\n",
      "         -9.4950e-01,  1.0887e-01,  1.0000e+00,  6.0751e-02,  8.3376e-02,\n",
      "         -3.1304e-03,  8.5578e-02, -3.1288e-01,  2.6283e-01,  2.6870e-01,\n",
      "         -1.4267e-01, -7.4000e-01,  2.2857e-01, -7.9442e-01, -9.8812e-01,\n",
      "          4.3592e-01,  7.7230e-02, -3.8084e-02,  9.9490e-01,  3.2616e-01,\n",
      "          6.7990e-02,  8.2889e-02,  4.7391e-01, -2.1855e-01,  3.9278e-01,\n",
      "          3.7667e-02,  9.6440e-01, -1.8374e-01,  3.9259e-01,  4.3319e-01,\n",
      "         -1.8618e-01, -2.1584e-01, -4.9610e-01, -9.7025e-02, -8.8006e-01,\n",
      "          2.4995e-01, -9.3940e-01,  9.3827e-01,  3.2001e-01,  1.1919e-01,\n",
      "          7.3959e-02,  3.1274e-02,  1.0000e+00, -7.5632e-01,  3.5396e-01,\n",
      "          5.3290e-01,  3.2036e-01, -9.7538e-01, -4.7482e-01, -2.3322e-01,\n",
      "          3.5376e-02, -4.6061e-02, -1.2863e-01,  8.3798e-02, -9.5139e-01,\n",
      "          3.4664e-02,  4.5233e-03, -8.8296e-01, -9.8300e-01,  1.6467e-01,\n",
      "          3.3595e-01, -1.0217e-01, -7.0275e-01, -4.3307e-01, -5.4169e-01,\n",
      "          1.8884e-01, -5.5797e-02, -9.2162e-01,  4.4790e-01, -3.5256e-02,\n",
      "          2.1131e-01, -4.6267e-02,  4.1688e-01,  1.9312e-01,  8.2643e-01,\n",
      "          3.1895e-02,  1.8035e-02,  2.2502e-02, -5.6261e-01,  5.2690e-01,\n",
      "         -4.1523e-01, -2.0335e-01,  5.0975e-03,  1.0000e+00, -1.3769e-01,\n",
      "          4.0090e-01,  4.8581e-01,  3.0547e-01,  1.0161e-01,  1.1372e-01,\n",
      "          5.4688e-01,  1.7282e-01, -1.1611e-01,  1.1691e-01,  3.3706e-01,\n",
      "         -9.4996e-02,  3.3125e-01, -1.1599e-01,  5.5664e-02,  6.9017e-01,\n",
      "          5.2775e-01, -7.8248e-02,  7.7874e-02, -2.5570e-01,  9.5441e-01,\n",
      "          4.4725e-02,  7.5062e-02, -1.6521e-01,  9.8572e-02, -1.2673e-01,\n",
      "          4.2396e-01,  9.9999e-01,  1.4012e-01, -6.5116e-02, -9.8683e-01,\n",
      "         -3.4660e-01, -6.9549e-01,  9.9968e-01,  7.8693e-01, -6.2560e-01,\n",
      "          4.0561e-01,  5.1398e-01, -7.1927e-03,  3.7469e-01, -4.9920e-02,\n",
      "         -1.8379e-01,  1.0699e-01,  6.4272e-02,  9.4363e-01, -4.5982e-01,\n",
      "         -9.6684e-01, -4.8714e-01,  1.6233e-01, -9.2982e-01,  9.8976e-01,\n",
      "         -2.8241e-01, -3.9526e-02, -2.8969e-01,  2.2178e-01, -7.3322e-01,\n",
      "         -1.9752e-01, -9.7385e-01,  1.4625e-01,  1.7385e-02,  9.4459e-01,\n",
      "          8.0070e-02, -4.1026e-01, -7.2363e-01,  6.5496e-02,  2.9531e-01,\n",
      "         -2.0402e-01, -9.4453e-01,  9.4867e-01, -9.6224e-01,  4.1987e-01,\n",
      "          9.9992e-01,  2.0182e-01, -5.9719e-01,  6.7062e-02, -1.3560e-01,\n",
      "          1.1140e-01, -7.1073e-02,  3.3843e-01, -9.1928e-01, -1.1785e-01,\n",
      "          7.1899e-03,  9.3813e-02,  1.2718e-01, -4.2176e-01,  6.2383e-01,\n",
      "         -3.0948e-02, -3.9573e-01, -4.9911e-01,  1.9713e-01,  1.9574e-01,\n",
      "          5.2774e-01, -6.4999e-02,  3.8217e-02, -1.3764e-01,  1.3114e-01,\n",
      "         -8.2896e-01, -6.2802e-02, -1.3078e-01, -9.9745e-01,  3.8189e-01,\n",
      "         -1.0000e+00, -4.9527e-02, -3.3011e-01, -9.7046e-03,  7.4031e-01,\n",
      "          4.5588e-01, -4.3037e-02, -5.9485e-01,  3.5137e-02,  8.4290e-01,\n",
      "          7.0024e-01,  4.9499e-03,  1.5221e-01, -4.8182e-01,  3.4913e-02,\n",
      "          6.8681e-02,  5.9797e-02,  9.4146e-02,  5.7532e-01,  3.5063e-02,\n",
      "          1.0000e+00, -4.4781e-03, -3.4757e-01, -7.9309e-01,  5.7241e-02,\n",
      "         -4.8242e-02,  9.9991e-01, -3.6963e-01, -9.2729e-01,  2.2610e-01,\n",
      "         -3.2602e-01, -6.5948e-01,  2.3506e-01, -6.6026e-02, -6.2875e-01,\n",
      "         -4.7124e-01,  8.3105e-01,  4.3462e-01, -5.2237e-01,  2.1811e-01,\n",
      "         -1.1176e-01, -2.7027e-01, -6.8502e-02,  5.0505e-02,  9.8319e-01,\n",
      "          3.3888e-01,  5.6442e-01,  1.0517e-01,  6.1440e-02,  9.3666e-01,\n",
      "          7.3989e-02, -2.4528e-01, -8.5207e-02,  9.9998e-01,  1.4210e-01,\n",
      "         -8.2488e-01,  2.2405e-01, -9.2098e-01, -1.0235e-01, -8.4105e-01,\n",
      "          2.1140e-01, -3.4106e-02,  8.0942e-01,  4.9840e-03,  8.9624e-01,\n",
      "          6.7184e-02, -1.7137e-01, -2.7561e-01,  2.6385e-01,  1.9073e-01,\n",
      "         -8.6307e-01, -9.8238e-01, -9.8035e-01,  2.2370e-01, -3.5154e-01,\n",
      "          1.9181e-01,  8.9503e-02, -9.8139e-02,  8.3593e-02,  3.0373e-01,\n",
      "         -9.9998e-01,  9.0944e-01,  2.9007e-01,  4.4585e-01,  9.4631e-01,\n",
      "          4.1260e-01,  1.9621e-01,  2.4693e-01, -9.7562e-01, -7.6957e-01,\n",
      "         -1.7996e-01, -5.8601e-02,  4.2949e-01,  3.3341e-01,  8.0547e-01,\n",
      "          2.5306e-01, -4.0736e-01, -3.4586e-02,  4.0999e-01, -8.3874e-01,\n",
      "         -9.9092e-01,  3.0937e-01,  3.3917e-01, -6.2679e-01,  9.4565e-01,\n",
      "         -5.9613e-01, -1.9441e-03,  3.7971e-01, -2.2250e-01,  5.2158e-01,\n",
      "          5.9324e-01, -1.8357e-02, -6.8000e-03,  2.1554e-01,  8.2484e-01,\n",
      "          8.0068e-01,  9.7795e-01, -1.0868e-01,  4.3963e-01,  2.2388e-01,\n",
      "          2.7078e-01,  8.5065e-01, -9.2567e-01,  4.3629e-03, -3.2062e-02,\n",
      "         -1.9565e-01,  1.1169e-01, -9.4711e-02, -7.2644e-01,  6.3986e-01,\n",
      "         -1.7955e-01,  4.2939e-01, -2.0787e-01,  2.2294e-01, -2.3857e-01,\n",
      "          6.7195e-02, -5.1772e-01, -3.6389e-01,  5.3169e-01,  5.3484e-02,\n",
      "          8.5309e-01,  6.4611e-01,  1.2341e-02, -2.4756e-01,  1.4718e-02,\n",
      "         -5.3294e-02, -9.2566e-01,  5.0771e-01,  1.2492e-01,  2.1457e-01,\n",
      "         -6.7958e-02, -2.7113e-01,  9.0946e-01, -1.9032e-01, -2.1274e-01,\n",
      "         -6.4847e-02, -4.3871e-01,  6.3751e-01, -2.1017e-01, -2.9291e-01,\n",
      "         -3.1616e-01,  5.4117e-01,  1.6768e-01,  9.9424e-01, -9.4509e-02,\n",
      "         -2.9022e-01, -2.1885e-03, -1.5720e-01,  2.8317e-01, -2.9364e-01,\n",
      "         -9.9998e-01,  1.4066e-01,  9.1604e-02,  1.1458e-01, -2.1965e-01,\n",
      "          3.0746e-01, -5.7720e-02, -8.7692e-01, -9.3892e-02,  2.2809e-01,\n",
      "          3.8768e-02, -3.2828e-01, -3.1139e-01,  4.1117e-01,  4.6004e-01,\n",
      "          5.5266e-01,  7.2535e-01,  2.5635e-01,  5.2958e-01,  4.7964e-01,\n",
      "         -1.0402e-01, -5.4204e-01,  8.4934e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79880111",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Paris is the capital of france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf27728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-239259da9fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6737f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91183f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
      "softmax torch: tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy:', outputs)\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0) # along values along first axis\n",
    "print('softmax torch:', outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591702ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(actual, predicted):\n",
    "    EPS = 1e-15\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afff2462",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9c7d9083178b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20e0fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([1, 0, 0])\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "print(f'Loss2 numpy: {l2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe103090",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor([0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da58546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor([0])\n",
    "loss = nn.CrossEntropyLoss()\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "068e3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1: 0.4170\n",
      "PyTorch Loss2: 1.8406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f}')\n",
    "print(f'PyTorch Loss2: {l2.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a27749a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class: 0, Y_pred1: 0, Y_pred2: 1\n"
     ]
    }
   ],
   "source": [
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "print(f'Actual class: {Y.item()}, Y_pred1: {predictions1.item()}, Y_pred2: {predictions2.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81e2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting attention\n",
      "  Downloading attention-4.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from attention) (1.19.5)\n",
      "Requirement already satisfied: tensorflow>=2.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from attention) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (0.37.0)\n",
      "Requirement already satisfied: keras~=2.6 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (2.6.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.12.1)\n",
      "Requirement already satisfied: clang~=5.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (3.7.4.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.41.1)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (2.7.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.1.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (0.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (3.3.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\Users\\HP\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: six~=1.15.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.15.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (3.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (2.7.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (3.19.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow>=2.1->attention) (1.1.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (2.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (58.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow>=2.1->attention) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1->attention) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1->attention) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1->attention) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1->attention) (1.3.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1->attention) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1->attention) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1->attention) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1->attention) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1->attention) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in e:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1->attention) (3.1.1)\n",
      "Installing collected packages: attention\n",
      "Successfully installed attention-4.0\n"
     ]
    }
   ],
   "source": [
    "pip install attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61015e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21194156 0.57611688 0.21194156]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# example of a function for calculating softmax for a list of numbers\n",
    "from numpy import exp\n",
    "\n",
    "# calculate the softmax of a vector\n",
    "def softmax(vector):\n",
    "\te = exp(vector)\n",
    "\treturn e / e.sum()\n",
    "\n",
    "# define data\n",
    "data = [2, 3, 2]\n",
    "# convert list of numbers to a list of probabilities\n",
    "result = softmax(data)\n",
    "# report the probabilities\n",
    "print(result)\n",
    "# report the sum of the probabilities\n",
    "print(sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d080d472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-0.5365, -0.1575, -0.7642,  ...,  0.2975, -0.2024, -0.7189],\n",
      "          [ 0.0785, -0.6052, -0.3328,  ..., -0.6899, -0.7451, -1.4309],\n",
      "          [ 0.1157, -1.1265,  0.3560,  ...,  0.6098, -0.1143, -0.1270],\n",
      "          ...,\n",
      "          [ 1.0998,  1.5778, -1.6758,  ...,  0.2394, -0.0418, -0.1995],\n",
      "          [-1.0527, -1.3451,  0.7468,  ...,  1.1905,  0.4901,  0.1118],\n",
      "          [ 0.3609,  0.1165,  0.0045,  ..., -0.0366,  2.4399, -0.5892]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8165,  0.4562, -0.1476,  ..., -1.2129, -0.5387,  0.4580],\n",
      "          [ 0.0367, -0.5528,  1.2393,  ...,  0.2391,  0.0750, -0.8457],\n",
      "          [ 0.1633,  0.0947,  1.4896,  ...,  1.1142, -0.0283, -1.0649],\n",
      "          ...,\n",
      "          [ 1.5199, -0.5769, -1.1874,  ...,  0.4412,  0.4744,  1.3841],\n",
      "          [ 0.2516,  0.3910, -0.2571,  ...,  1.4424,  0.6642,  0.8867],\n",
      "          [-0.2385, -0.3781, -0.4113,  ...,  0.2637, -0.4387,  0.2785]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9458,  1.0127, -0.2874,  ..., -1.2522,  0.6903,  0.1019],\n",
      "          [ 0.0601, -0.2136,  0.2581,  ..., -0.7442,  0.2280,  0.3678],\n",
      "          [ 0.4355, -0.8670, -0.0459,  ...,  0.3126, -0.7155,  0.4446],\n",
      "          ...,\n",
      "          [-0.9595, -0.9507,  0.0196,  ...,  0.0149, -0.4093, -0.7996],\n",
      "          [ 0.2366, -0.6097,  1.1732,  ...,  0.4377, -0.0294, -0.3422],\n",
      "          [-0.7148,  0.4806, -0.7397,  ...,  0.2529,  0.4979,  0.7309]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0178, -0.1759, -0.7904,  ..., -0.2605, -0.7919, -0.2654],\n",
      "          [ 0.9923, -0.7509,  0.4349,  ..., -1.0463,  0.3888,  0.0684],\n",
      "          [ 0.2464, -0.2542,  0.0559,  ...,  1.3503,  1.0904,  0.6526],\n",
      "          ...,\n",
      "          [-0.1123,  0.3475, -0.1181,  ...,  0.3246, -0.2976,  0.0661],\n",
      "          [ 0.1881,  0.6336,  0.4655,  ...,  0.4077, -0.5360,  0.0453],\n",
      "          [ 0.4158, -0.1651,  0.8640,  ..., -0.8424,  0.3950,  0.0559]]],\n",
      "\n",
      "\n",
      "        [[[-1.0592,  0.0709,  0.1139,  ...,  0.2559, -0.1315, -0.1545],\n",
      "          [ 1.4327, -0.1891,  0.0499,  ...,  0.7620, -0.4890,  1.1087],\n",
      "          [ 0.0037,  1.2589, -1.4753,  ..., -0.6725,  0.2164,  0.6344],\n",
      "          ...,\n",
      "          [-0.2072, -0.0930, -0.4938,  ...,  0.2530,  0.1774,  0.0736],\n",
      "          [-0.1144, -0.1884,  0.5215,  ..., -0.1238,  1.4484,  0.7170],\n",
      "          [ 0.0893, -0.0911,  0.9042,  ..., -0.4083,  0.0338,  0.4507]]],\n",
      "\n",
      "\n",
      "        [[[-0.3989,  1.0294, -0.8418,  ..., -1.3218, -1.4580,  0.4058],\n",
      "          [ 0.5701, -0.5335, -1.0763,  ...,  0.1175,  1.6193, -0.4503],\n",
      "          [ 0.9366, -0.3308, -0.0134,  ...,  0.1544,  0.6954, -0.3277],\n",
      "          ...,\n",
      "          [ 0.5134, -0.7422,  0.5501,  ..., -0.5945,  0.4477, -0.3491],\n",
      "          [-0.4178,  0.2879,  0.5817,  ...,  0.5211, -0.6523, -0.6157],\n",
      "          [-1.3391,  1.1026,  1.1004,  ...,  0.6168, -0.0818,  0.7553]]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[-4.1495e-01,  2.4733e-01, -1.2708e-02],\n",
      "        [-4.0163e-01,  2.3615e-01, -2.2369e-02],\n",
      "        [-4.1097e-01,  2.4432e-01, -7.9118e-04],\n",
      "        [-4.0592e-01,  2.3291e-01, -1.1211e-02],\n",
      "        [-3.9735e-01,  2.2773e-01, -1.2652e-02],\n",
      "        [-3.8077e-01,  2.3198e-01,  6.6526e-03],\n",
      "        [-4.0293e-01,  2.2578e-01,  2.1848e-04],\n",
      "        [-3.9141e-01,  2.0563e-01, -1.4815e-02],\n",
      "        [-4.2286e-01,  2.4147e-01, -1.8551e-02],\n",
      "        [-4.0054e-01,  2.2605e-01, -1.6273e-02],\n",
      "        [-3.9242e-01,  2.3066e-01, -1.2130e-02],\n",
      "        [-4.3007e-01,  2.4197e-01, -1.8488e-02],\n",
      "        [-3.8492e-01,  2.3979e-01, -8.4659e-03],\n",
      "        [-4.1177e-01,  2.1999e-01, -3.2941e-02],\n",
      "        [-4.0826e-01,  2.3884e-01, -2.5319e-02],\n",
      "        [-4.0747e-01,  2.2760e-01,  1.4245e-02]], grad_fn=<SumBackward1>))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-15da00ad97d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mtemporal_block\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTemporalAttn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemporal_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "Attention blocks\n",
    "Reference: Learn To Pay Attention\n",
    "\"\"\"\n",
    "class ProjectorBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProjectorBlock, self).__init__()\n",
    "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features,\n",
    "            kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class SpatialAttn(nn.Module):\n",
    "    def __init__(self, in_features, normalize_attn=True):\n",
    "        super(SpatialAttn, self).__init__()\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1,\n",
    "            kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, l, g):\n",
    "        N, C, H, W = l.size()\n",
    "        c = self.op(l+g) # (batch_size,1,H,W)\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,H,W)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        g = torch.mul(a.expand_as(l), l)\n",
    "        if self.normalize_attn:\n",
    "            g = g.view(N,C,-1).sum(dim=2) # (batch_size,C)\n",
    "        else:\n",
    "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
    "        return c.view(N,1,H,W), g\n",
    "\n",
    "\"\"\"\n",
    "Temporal attention block\n",
    "Reference: https://github.com/philipperemy/keras-attention-mechanism\n",
    "\"\"\"\n",
    "class TemporalAttn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TemporalAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc2 = nn.Linear(self.hidden_size*2, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # (batch_size, time_steps, hidden_size)\n",
    "        score_first_part = self.fc1(hidden_states)\n",
    "        # (batch_size, hidden_size)\n",
    "        h_t = hidden_states[:,-1,:]\n",
    "        # (batch_size, time_steps)\n",
    "        score = torch.bmm(score_first_part, h_t.unsqueeze(2)).squeeze(2)\n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "        # (batch_size, hidden_size)\n",
    "        context_vector = torch.bmm(hidden_states.permute(0,2,1), attention_weights.unsqueeze(2)).squeeze(2)\n",
    "        # (batch_size, hidden_size*2)\n",
    "        pre_activation = torch.cat((context_vector, h_t), dim=1)\n",
    "        # (batch_size, hidden_size)\n",
    "        attention_vector = self.fc2(pre_activation)\n",
    "        attention_vector = torch.tanh(attention_vector)\n",
    "\n",
    "        return attention_vector, attention_weights\n",
    "\n",
    "# Test\n",
    "if __name__ == '__main__':\n",
    "    # 2d block\n",
    "    spatial_block = SpatialAttn(in_features=3)\n",
    "    l = torch.randn(16, 3, 128, 128)\n",
    "    g = torch.randn(16, 3, 128, 128)\n",
    "    print(spatial_block(l, g))\n",
    "    # temporal block\n",
    "    temporal_block = TemporalAttn(hidden_size=256)\n",
    "    x = torch.randn(16, 30, 256)\n",
    "    print(temporal_block(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcdab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 10, 64)       16896       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "last_hidden_state (Lambda)      (None, 64)           0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "attention_score_vec (Dense)     (None, 10, 64)       4096        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "attention_score (Dot)           (None, 10)           0           last_hidden_state[0][0]          \n",
      "                                                                 attention_score_vec[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_weight (Activation)   (None, 10)           0           attention_score[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "context_vector (Dot)            (None, 64)           0           lstm[0][0]                       \n",
      "                                                                 attention_weight[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_output (Concatenate)  (None, 128)          0           context_vector[0][0]             \n",
      "                                                                 last_hidden_state[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "attention_vector (Dense)        (None, 128)          16384       attention_output[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            129         attention_vector[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 37,505\n",
      "Trainable params: 37,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 3s 9ms/step - loss: 0.3789\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2585\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2583\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2467\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2503\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2574\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2514\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2447\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2465\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2456\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from attention import Attention\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Dummy data. There is nothing to learn in this example.\n",
    "    num_samples, time_steps, input_dim, output_dim = 100, 10, 1, 1\n",
    "    data_x = np.random.uniform(size=(num_samples, time_steps, input_dim))\n",
    "    data_y = np.random.uniform(size=(num_samples, output_dim))\n",
    "\n",
    "    # Define/compile the model.\n",
    "    model_input = Input(shape=(time_steps, input_dim))\n",
    "    x = LSTM(64, return_sequences=True)(model_input)\n",
    "    x = Attention(32)(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(model_input, x)\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    print(model.summary())\n",
    "\n",
    "    # train.\n",
    "    model.fit(data_x, data_y, epochs=10)\n",
    "\n",
    "    # test save/reload model.\n",
    "    pred1 = model.predict(data_x)\n",
    "    model.save('test_model.h5')\n",
    "    model_h5 = load_model('test_model.h5')\n",
    "    pred2 = model_h5.predict(data_x)\n",
    "    np.testing.assert_almost_equal(pred1, pred2)\n",
    "    print('Success.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb5afde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-766197ade825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_pad_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_subsequent_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformer'"
     ]
    }
   ],
   "source": [
    "''' This module will handle the text generation with beam search. '''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformer.Models import Transformer, get_pad_mask, get_subsequent_mask\n",
    "\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    ''' Load a trained model and translate in beam search fashion. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, model, beam_size, max_seq_len,\n",
    "            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n",
    "        \n",
    "\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.alpha = 0.7\n",
    "        self.beam_size = beam_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_bos_idx = trg_bos_idx\n",
    "        self.trg_eos_idx = trg_eos_idx\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n",
    "        self.register_buffer(\n",
    "            'blank_seqs', \n",
    "            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n",
    "        self.blank_seqs[:, 0] = self.trg_bos_idx\n",
    "        self.register_buffer(\n",
    "            'len_map', \n",
    "            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n",
    "\n",
    "\n",
    "    def _model_decode(self, trg_seq, enc_output, src_mask):\n",
    "        trg_mask = get_subsequent_mask(trg_seq)\n",
    "        dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1)\n",
    "\n",
    "\n",
    "    def _get_init_state(self, src_seq, src_mask):\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n",
    "        dec_output = self._model_decode(self.init_seq, enc_output, src_mask)\n",
    "        \n",
    "        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        scores = torch.log(best_k_probs).view(beam_size)\n",
    "        gen_seq = self.blank_seqs.clone().detach()\n",
    "        gen_seq[:, 1] = best_k_idx[0]\n",
    "        enc_output = enc_output.repeat(beam_size, 1, 1)\n",
    "        return enc_output, gen_seq, scores\n",
    "\n",
    "\n",
    "    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n",
    "        assert len(scores.size()) == 1\n",
    "        \n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        # Get k candidates for each beam, k^2 candidates in total.\n",
    "        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        # Include the previous scores.\n",
    "        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n",
    "\n",
    "        # Get the best k candidates from k^2 candidates.\n",
    "        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n",
    " \n",
    "        # Get the corresponding positions of the best k candidiates.\n",
    "        best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n",
    "        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n",
    "\n",
    "        # Copy the corresponding previous tokens.\n",
    "        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n",
    "        # Set the best tokens in this beam search step\n",
    "        gen_seq[:, step] = best_k_idx\n",
    "\n",
    "        return gen_seq, scores\n",
    "\n",
    "\n",
    "    def translate_sentence(self, src_seq):\n",
    "        # Only accept batch size equals to 1 in this function.\n",
    "        # TODO: expand to batch operation.\n",
    "        assert src_seq.size(0) == 1\n",
    "\n",
    "        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n",
    "        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n",
    "\n",
    "        with torch.no_grad():\n",
    "            src_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "            enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n",
    "\n",
    "            ans_idx = 0   # default\n",
    "            for step in range(2, max_seq_len):    # decode up to max length\n",
    "                dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n",
    "                gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n",
    "\n",
    "                # Check if all path finished\n",
    "                # -- locate the eos in the generated sequences\n",
    "                eos_locs = gen_seq == trg_eos_idx   \n",
    "                # -- replace the eos with its position for the length penalty use\n",
    "                seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n",
    "                # -- check if all beams contain eos\n",
    "                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n",
    "                    # TODO: Try different terminate conditions.\n",
    "                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n",
    "                    ans_idx = ans_idx.item()\n",
    "                    break\n",
    "        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9fae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\n",
      "ERROR: No matching distribution found for transformer\n",
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\Users\\HP\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96379bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement transformer[torch] (from versions: none)\n",
      "ERROR: No matching distribution found for transformer[torch]\n",
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\Users\\HP\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformer[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "154d7f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-060e82658acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "batch = np.asarray([[1,2,3,6],[2,4,5,6],[1,2,3,6]])\n",
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y = tf.nn.softmax(x)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(y, feed_dict={x: batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d814c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution is: [0.95257413 0.04742587]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector=np.array([6.0,3.0])\n",
    "exp=np.exp(vector)\n",
    "probability=exp/np.sum(exp)\n",
    "print(\"Probability distribution is:\",probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda2b671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sig(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "def softmax_cross_entropy(z,y):\n",
    "    if y==1:\n",
    "        return -np.log(z)\n",
    "    else:\n",
    "        return -np.log(1-z)\n",
    "x=np.arange(-9,9,0.1)\n",
    "a=sig(x)\n",
    "softmax1=softmax_cross_entropy(a,1)\n",
    "softmax2=softmax_cross_entropy(a,0)\n",
    "figure,axis=plt.subplots(figsize=(7,7))\n",
    "plt.plot(a,softmax1)\n",
    "plt.plot(a,softmax2)\n",
    "plt.xlabel(\"Cross entropy loss\")\n",
    "plt.ylabel(\"log loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62afa358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
